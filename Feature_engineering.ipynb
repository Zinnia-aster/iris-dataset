{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Handling missing values is crucial to ensure the dataset remains reliable. Common techniques include imputation, where missing values are replaced with the mean, median, or mode of the respective feature, and dropping, where rows or columns with excessive missing values are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before introducing NaNs: 0\n",
      "✅ All missing values have been handled. No missing values remain.\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1          3.049655                1.4               0.2\n",
      "1                4.9          3.049655                1.4               0.2\n",
      "2                4.7          3.049655                1.3               0.2\n",
      "3                4.6          3.049655                1.5               0.2\n",
      "4                5.0          3.049655                1.4               0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "print(\"Missing values before introducing NaNs:\", df.isnull().sum().sum())\n",
    "# Simulate missing values (for practice)\n",
    "df.iloc[0:5, 1] = np.nan  # Introduce NaN values in column 1\n",
    "\n",
    "# Fill missing values with the column mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df.iloc[:, :] = imputer.fit_transform(df)\n",
    "\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"✅ All missing values have been handled. No missing values remain.\")\n",
    "else:\n",
    "    print(\"⚠️ There are still missing values in the dataset.\")\n",
    "\n",
    "print(df.head())  # Check if missing values are handled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (standardizatin & normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models perform better when features are on a similar scale. Standardization (zero mean, unit variance) and Normalization (scaling to [0,1]) prevent larger features from dominating. This is crucial for distance-based models like KNN and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data  # Extract features\n",
    "y = data.target  # Target variable (species)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "\n",
    "print(X_scaled[:5])  # Display first 5 rows after scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all features contribute equally to predictions, and irrelevant ones can lead to overfitting. Variance Threshold removes low-variance features, while SelectKBest picks the most informative ones using statistical tests. This optimizes model performance and reduces computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)  # Select top 2 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_selected[:5])  # Display selected features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating polynomial selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models may not capture complex patterns in data. Polynomial features introduce squared, cubic, or higher-order terms to improve flexibility. While this can enhance accuracy, too many features may cause overfitting, so careful selection is necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(X_poly.shape)  # New feature set size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
